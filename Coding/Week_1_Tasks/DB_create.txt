from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from datetime import date, timedelta
import random

# Create SparkSession
spark = SparkSession.builder \
    .appName("Expanded Sample Dataset") \
    .getOrCreate()

# Helper function to generate dates
def random_date(start_date=date(2023, 1, 1), end_date=date(2023, 12, 31)):
    time_between_dates = end_date - start_date
    days_between_dates = time_between_dates.days
    random_number_of_days = random.randrange(days_between_dates)
    return start_date + timedelta(days=random_number_of_days)

# Create CATEGORIES DataFrame (expanded to 8 categories)
categories_data = [
    (1, 'Electronics', 'Electronic devices'),
    (2, 'Clothing', 'Apparel and accessories'),
    (3, 'Home Goods', 'Household items'),
    (4, 'Sports', 'Sporting equipment'),
    (5, 'Books', 'Literature and educational materials'),
    (6, 'Kitchenware', 'Cooking utensils and appliances'),
    (7, 'Beauty', 'Cosmetics and personal care'),
    (8, 'Outdoor', 'Camping and outdoor gear')
]

categories_df = spark.createDataFrame(
    categories_data,
    ['id', 'name', 'description']
)

# Create PRODUCTS DataFrame (expanded to 25 products)
products_data = []
product_names = [
    ('Smartphone', 1), ('Laptop', 1), ('Headphones', 1),
    ('Tablet', 1), ('Smartwatch', 1), ('Power Bank', 1),
    ('T-Shirt', 2), ('Jeans', 2), ('Shoes', 2),
    ('Jacket', 2), ('Hat', 2), ('Scarf', 2),
    ('Sofa', 3), ('Bed', 3), ('Chair', 3),
    ('Table', 3), ('Desk', 3), ('Bookshelf', 3),
    ('Basketball', 4), ('Football', 4), ('Tennis Racket', 4),
    ('Running Shoes', 4), ('Golf Clubs', 4), ('Skates', 4)
]

prices = {
    1: [599.99, 999.99, 149.99, 299.99, 199.99, 29.99],
    2: [29.99, 79.99, 89.99, 99.99, 39.99, 19.99],
    3: [899.99, 599.99, 299.99, 499.99, 799.99, 399.99],
    4: [49.99, 79.99, 99.99, 129.99, 299.99, 199.99]
}

for i, (name, category_id) in enumerate(product_names, 1):
    price = random.choice(prices[category_id])
    status = 'active' if random.random() > 0.1 else 'inactive'
    products_data.append((i, name, category_id, price, status))

products_df = spark.createDataFrame(
    products_data,
    ['id', 'name', 'category_id', 'base_price', 'status']
)

# Create CUSTOMERS DataFrame (expanded to 25 customers)
countries = ['USA', 'Canada', 'UK', 'Australia', 'Germany', 'France', 'Japan', 'India']
customer_names = [
    ('John Smith', 'john.smith@email.com'),
    ('Jane Doe', 'jane.doe@email.com'),
    ('Bob Wilson', 'bob.wilson@email.com'),
    ('Alice Brown', 'alice.brown@email.com'),
    ('Mike Davis', 'mike.davis@email.com')
]

customers_data = []
for i in range(1, 26):
    base_name = customer_names[i % len(customer_names)]
    email = f"{base_name[1]}_{i}"
    country = random.choice(countries)
    status = 'active' if random.random() > 0.1 else 'inactive'
    join_date = random_date(date(2023, 1, 1), date(2023, 12, 31))
    customers_data.append((i, f"{base_name[0]}_{i}", country, status, join_date))

customers_df = spark.createDataFrame(
    customers_data,
    ['id', 'name', 'country', 'status', 'join_date']
)

# Create ORDERS DataFrame (expanded to 50 orders)
orders_data = []
order_statuses = ['completed', 'pending', 'cancelled']

for i in range(1, 51):
    customer_id = random.randint(1, 25)
    order_date = random_date(date(2023, 1, 1), date(2023, 12, 31))
    total_amount = round(random.uniform(50.0, 5000.0), 2)
    status = random.choice(order_statuses)
    orders_data.append((i, customer_id, order_date, total_amount, status))

orders_df = spark.createDataFrame(
    orders_data,
    ['id', 'customer_id', 'order_date', 'total_amount', 'status']
)

# Create ORDER_ITEMS DataFrame (expanded to 100 items)
order_items_data = []
product_ids = list(range(1, 26))

for i in range(1, 101):
    order_id = random.randint(1, 50)
    product_id = random.choice(product_ids)
    quantity = random.randint(1, 5)
    price = products_df.filter(col('id') == product_id)\
                      .collect()[0]['base_price']
    order_items_data.append((i, order_id, product_id, quantity, price))

order_items_df = spark.createDataFrame(
    order_items_data,
    ['id', 'order_id', 'product_id', 'quantity', 'price']
)

# Show record counts for each table
print("\nDataset Record Counts:")
print(f"CATEGORIES: {categories_df.count()} records")
print(f"PRODUCTS: {products_df.count()} records")
print(f"CUSTOMERS: {customers_df.count()} records")
print(f"ORDERS: {orders_df.count()} records")
print(f"ORDER_ITEMS: {order_items_df.count()} records")

# Save datasets to Parquet format
categories_df.write.parquet("data/categories_expanded")
products_df.write.parquet("data/products_expanded")
customers_df.write.parquet("data/customers_expanded")
orders_df.write.parquet("data/orders_expanded")
order_items_df.write.parquet("data/order_items_expanded")

# Show sample records from each table
print("\nSample Categories:")
categories_df.show()

print("\nSample Products:")
products_df.show()

print("\nSample Customers:")
customers_df.show()

print("\nSample Orders:")
orders_df.show()

print("\nSample Order Items:")
order_items_df.show()