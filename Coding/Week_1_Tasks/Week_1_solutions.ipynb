{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui5eZi6J-TQV"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clean installation sequence\n",
        "!apt-get purge openjdk-* -qq > /dev/null  # Remove existing Java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip uninstall pyspark -y -qq > /dev/null\n",
        "!pip install pyspark==3.5.0 -qq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Configure environment properly\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.11/dist-packages/pyspark\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Initialize Spark with proper paths\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ColabSparkFix\") \\\n",
        "    .config(\"spark.executor.memory\", \"1g\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlZoi2qiBiM1",
        "outputId": "efd49249-83e4-4291-e080-20ba4992b925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Record Counts:\n",
            "CATEGORIES: 8 records\n",
            "PRODUCTS: 25 records\n",
            "CUSTOMERS: 25 records\n",
            "ORDERS: 50 records\n",
            "ORDER_ITEMS: 100 records\n",
            "\n",
            "Sample Categories:\n",
            "+---+-----------+--------------------+\n",
            "| id|       name|         description|\n",
            "+---+-----------+--------------------+\n",
            "|  1|Electronics|  Electronic devices|\n",
            "|  2|   Clothing|Apparel and acces...|\n",
            "|  3| Home Goods|     Household items|\n",
            "|  4|     Sports|  Sporting equipment|\n",
            "|  5|      Books|Literature and ed...|\n",
            "|  6|Kitchenware|Cooking utensils ...|\n",
            "|  7|     Beauty|Cosmetics and per...|\n",
            "|  8|    Outdoor|Camping and outdo...|\n",
            "+---+-----------+--------------------+\n",
            "\n",
            "\n",
            "Sample Products:\n",
            "+---+----------+-----------+----------+--------+\n",
            "| id|      name|category_id|base_price|  status|\n",
            "+---+----------+-----------+----------+--------+\n",
            "|  1|Smartphone|          1|    199.99|  active|\n",
            "|  2|    Laptop|          1|    599.99|  active|\n",
            "|  3|Headphones|          1|    199.99|  active|\n",
            "|  4|    Tablet|          1|    149.99|  active|\n",
            "|  5|Smartwatch|          1|    199.99|  active|\n",
            "|  6|Power Bank|          1|    299.99|inactive|\n",
            "|  7|   T-Shirt|          2|     19.99|inactive|\n",
            "|  8|     Jeans|          2|     79.99|  active|\n",
            "|  9|     Shoes|          2|     79.99|  active|\n",
            "| 10|    Jacket|          2|     29.99|  active|\n",
            "| 11|       Hat|          2|     79.99|  active|\n",
            "| 12|     Scarf|          2|     19.99|  active|\n",
            "| 13|      Sofa|          3|    799.99|  active|\n",
            "| 14|       Bed|          3|    599.99|inactive|\n",
            "| 15|     Chair|          3|    499.99|  active|\n",
            "| 16|     Table|          3|    599.99|  active|\n",
            "| 17|      Desk|          3|    799.99|  active|\n",
            "| 18| Bookshelf|          3|    799.99|  active|\n",
            "| 19|Basketball|          4|     79.99|  active|\n",
            "| 20|  Football|          4|    299.99|  active|\n",
            "+---+----------+-----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Sample Customers:\n",
            "+---+--------------+---------+--------+----------+\n",
            "| id|          name|  country|  status| join_date|\n",
            "+---+--------------+---------+--------+----------+\n",
            "|  1|    Jane Doe_1|  Germany|  active|2023-02-13|\n",
            "|  2|  Bob Wilson_2|Australia|inactive|2023-12-08|\n",
            "|  3| Alice Brown_3|    India|  active|2023-12-28|\n",
            "|  4|  Mike Davis_4|      USA|  active|2023-08-08|\n",
            "|  5|  John Smith_5|       UK|  active|2023-09-07|\n",
            "|  6|    Jane Doe_6|    India|  active|2023-10-02|\n",
            "|  7|  Bob Wilson_7|    Japan|  active|2023-10-23|\n",
            "|  8| Alice Brown_8|   France|  active|2023-07-09|\n",
            "|  9|  Mike Davis_9|Australia|  active|2023-10-20|\n",
            "| 10| John Smith_10|    Japan|  active|2023-01-01|\n",
            "| 11|   Jane Doe_11|   Canada|  active|2023-06-22|\n",
            "| 12| Bob Wilson_12|   Canada|  active|2023-06-15|\n",
            "| 13|Alice Brown_13|   France|  active|2023-11-20|\n",
            "| 14| Mike Davis_14|       UK|  active|2023-08-30|\n",
            "| 15| John Smith_15|   France|inactive|2023-09-16|\n",
            "| 16|   Jane Doe_16|   Canada|  active|2023-07-07|\n",
            "| 17| Bob Wilson_17|      USA|  active|2023-10-15|\n",
            "| 18|Alice Brown_18|    India|  active|2023-06-27|\n",
            "| 19| Mike Davis_19|    Japan|  active|2023-06-10|\n",
            "| 20| John Smith_20|    Japan|  active|2023-04-12|\n",
            "+---+--------------+---------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Sample Orders:\n",
            "+---+-----------+----------+------------+---------+\n",
            "| id|customer_id|order_date|total_amount|   status|\n",
            "+---+-----------+----------+------------+---------+\n",
            "|  1|         19|2023-04-12|     1831.24|completed|\n",
            "|  2|         21|2023-04-11|     2655.56|  pending|\n",
            "|  3|          5|2023-08-03|     3858.49|cancelled|\n",
            "|  4|         22|2023-05-03|     3903.87|  pending|\n",
            "|  5|         11|2023-09-26|      390.74|  pending|\n",
            "|  6|         19|2023-03-05|     3047.48|completed|\n",
            "|  7|          6|2023-12-07|      899.91|completed|\n",
            "|  8|          4|2023-06-25|      962.03|cancelled|\n",
            "|  9|          2|2023-06-02|      446.72|  pending|\n",
            "| 10|          9|2023-02-22|      3053.4|  pending|\n",
            "| 11|          2|2023-11-29|     4170.33|cancelled|\n",
            "| 12|         22|2023-11-13|     2007.06|completed|\n",
            "| 13|         15|2023-03-17|     2853.24|  pending|\n",
            "| 14|          1|2023-06-01|     2722.47|completed|\n",
            "| 15|         24|2023-05-18|     4265.84|completed|\n",
            "| 16|         17|2023-12-27|      2634.2|  pending|\n",
            "| 17|         19|2023-03-06|     3419.64|completed|\n",
            "| 18|         12|2023-10-10|     4052.35|completed|\n",
            "| 19|         20|2023-09-22|     3619.82|  pending|\n",
            "| 20|         22|2023-05-27|      418.62|cancelled|\n",
            "+---+-----------+----------+------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Sample Order Items:\n",
            "+---+--------+----------+--------+------+\n",
            "| id|order_id|product_id|quantity| price|\n",
            "+---+--------+----------+--------+------+\n",
            "|  1|      30|        21|       3|299.99|\n",
            "|  2|      37|        10|       3| 29.99|\n",
            "|  3|      39|        23|       5|129.99|\n",
            "|  4|      35|         2|       5|599.99|\n",
            "|  5|      31|        14|       5|599.99|\n",
            "|  6|      25|        16|       3|599.99|\n",
            "|  7|       2|         3|       3|199.99|\n",
            "|  8|      13|        23|       3|129.99|\n",
            "|  9|      47|        25|       5|299.99|\n",
            "| 10|      33|         6|       4|299.99|\n",
            "| 11|      25|        14|       3|599.99|\n",
            "| 12|      36|        23|       2|129.99|\n",
            "| 13|      34|        14|       3|599.99|\n",
            "| 14|      30|         1|       2|199.99|\n",
            "| 15|      40|        13|       1|799.99|\n",
            "| 16|      36|        14|       5|599.99|\n",
            "| 17|      24|        14|       4|599.99|\n",
            "| 18|      48|         6|       4|299.99|\n",
            "| 19|       3|        19|       4| 79.99|\n",
            "| 20|      22|         9|       2| 79.99|\n",
            "+---+--------+----------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Saved files:\n",
            "total 20K\n",
            "drwxr-xr-x 2 root root 4.0K Mar 10 17:47 categories_expanded\n",
            "drwxr-xr-x 2 root root 4.0K Mar 10 17:47 customers_expanded\n",
            "drwxr-xr-x 2 root root 4.0K Mar 10 17:47 order_items_expanded\n",
            "drwxr-xr-x 2 root root 4.0K Mar 10 17:47 orders_expanded\n",
            "drwxr-xr-x 2 root root 4.0K Mar 10 17:47 products_expanded\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Create a database\n",
        "from pyspark.sql.functions import col\n",
        "from datetime import date, timedelta\n",
        "import random\n",
        "\n",
        "def random_date(start_date=date(2023, 1, 1), end_date=date(2023, 12, 31)):\n",
        "    time_between_dates = end_date - start_date\n",
        "    days_between_dates = time_between_dates.days\n",
        "    random_number_of_days = random.randrange(days_between_dates)\n",
        "    return start_date + timedelta(days=random_number_of_days)\n",
        "\n",
        "# Create CATEGORIES DataFrame\n",
        "categories_data = [\n",
        "    (1, 'Electronics', 'Electronic devices'),\n",
        "    (2, 'Clothing', 'Apparel and accessories'),\n",
        "    (3, 'Home Goods', 'Household items'),\n",
        "    (4, 'Sports', 'Sporting equipment'),\n",
        "    (5, 'Books', 'Literature and educational materials'),\n",
        "    (6, 'Kitchenware', 'Cooking utensils and appliances'),\n",
        "    (7, 'Beauty', 'Cosmetics and personal care'),\n",
        "    (8, 'Outdoor', 'Camping and outdoor gear')\n",
        "]\n",
        "\n",
        "categories_df = spark.createDataFrame(\n",
        "    categories_data,\n",
        "    ['id', 'name', 'description']\n",
        ")\n",
        "\n",
        "# Create PRODUCTS DataFrame\n",
        "products_data = []\n",
        "product_names = [\n",
        "    ('Smartphone', 1), ('Laptop', 1), ('Headphones', 1),\n",
        "    ('Tablet', 1), ('Smartwatch', 1), ('Power Bank', 1),\n",
        "    ('T-Shirt', 2), ('Jeans', 2), ('Shoes', 2),\n",
        "    ('Jacket', 2), ('Hat', 2), ('Scarf', 2),\n",
        "    ('Sofa', 3), ('Bed', 3), ('Chair', 3),\n",
        "    ('Table', 3), ('Desk', 3), ('Bookshelf', 3),\n",
        "    ('Basketball', 4), ('Football', 4), ('Tennis Racket', 4),\n",
        "    ('Running Shoes', 4), ('Golf Clubs', 4), ('Skates', 4),\n",
        "    ('Helmet', 4)  # Added 25th product\n",
        "]\n",
        "\n",
        "prices = {\n",
        "    1: [599.99, 999.99, 149.99, 299.99, 199.99, 29.99],\n",
        "    2: [29.99, 79.99, 89.99, 99.99, 39.99, 19.99],\n",
        "    3: [899.99, 599.99, 299.99, 499.99, 799.99, 399.99],\n",
        "    4: [49.99, 79.99, 99.99, 129.99, 299.99, 199.99]\n",
        "}\n",
        "\n",
        "for i, (name, category_id) in enumerate(product_names, 1):\n",
        "    price = random.choice(prices[category_id])\n",
        "    status = 'active' if random.random() > 0.1 else 'inactive'\n",
        "    products_data.append((i, name, category_id, price, status))\n",
        "\n",
        "products_df = spark.createDataFrame(\n",
        "    products_data,\n",
        "    ['id', 'name', 'category_id', 'base_price', 'status']\n",
        ")\n",
        "\n",
        "# Create CUSTOMERS DataFrame\n",
        "countries = ['USA', 'Canada', 'UK', 'Australia', 'Germany', 'France', 'Japan', 'India']\n",
        "customer_names = [\n",
        "    ('John Smith', 'john.smith@email.com'),\n",
        "    ('Jane Doe', 'jane.doe@email.com'),\n",
        "    ('Bob Wilson', 'bob.wilson@email.com'),\n",
        "    ('Alice Brown', 'alice.brown@email.com'),\n",
        "    ('Mike Davis', 'mike.davis@email.com')\n",
        "]\n",
        "\n",
        "customers_data = []\n",
        "for i in range(1, 26):\n",
        "    base_name = customer_names[i % len(customer_names)]\n",
        "    email = f\"{base_name[1]}_{i}\"\n",
        "    country = random.choice(countries)\n",
        "    status = 'active' if random.random() > 0.1 else 'inactive'\n",
        "    join_date = random_date(date(2023, 1, 1), date(2023, 12, 31))\n",
        "    customers_data.append((i, f\"{base_name[0]}_{i}\", country, status, join_date))\n",
        "\n",
        "customers_df = spark.createDataFrame(\n",
        "    customers_data,\n",
        "    ['id', 'name', 'country', 'status', 'join_date']\n",
        ")\n",
        "\n",
        "# Create ORDERS DataFrame\n",
        "orders_data = []\n",
        "order_statuses = ['completed', 'pending', 'cancelled']\n",
        "\n",
        "for i in range(1, 51):\n",
        "    customer_id = random.randint(1, 25)\n",
        "    order_date = random_date(date(2023, 1, 1), date(2023, 12, 31))\n",
        "    total_amount = round(random.uniform(50.0, 5000.0), 2)\n",
        "    status = random.choice(order_statuses)\n",
        "    orders_data.append((i, customer_id, order_date, total_amount, status))\n",
        "\n",
        "orders_df = spark.createDataFrame(\n",
        "    orders_data,\n",
        "    ['id', 'customer_id', 'order_date', 'total_amount', 'status']\n",
        ")\n",
        "\n",
        "# Create ORDER_ITEMS DataFrame with valid product IDs\n",
        "valid_product_ids = [row.id for row in products_df.select('id').collect()]\n",
        "\n",
        "order_items_data = []\n",
        "for i in range(1, 101):\n",
        "    order_id = random.randint(1, 50)\n",
        "    product_id = random.choice(valid_product_ids)\n",
        "    quantity = random.randint(1, 5)\n",
        "\n",
        "    product_row = products_df.filter(col('id') == product_id).first()\n",
        "    if product_row:\n",
        "        price = product_row['base_price']\n",
        "    else:\n",
        "        price = 0.0  # Fallback value or handle differently\n",
        "        continue      # Skip invalid items\n",
        "\n",
        "    order_items_data.append((i, order_id, product_id, quantity, price))\n",
        "\n",
        "order_items_df = spark.createDataFrame(\n",
        "    order_items_data,\n",
        "    ['id', 'order_id', 'product_id', 'quantity', 'price']\n",
        ")\n",
        "\n",
        "# Show record counts for each table\n",
        "print(\"\\nDataset Record Counts:\")\n",
        "print(f\"CATEGORIES: {categories_df.count()} records\")\n",
        "print(f\"PRODUCTS: {products_df.count()} records\")\n",
        "print(f\"CUSTOMERS: {customers_df.count()} records\")\n",
        "print(f\"ORDERS: {orders_df.count()} records\")\n",
        "print(f\"ORDER_ITEMS: {order_items_df.count()} records\")\n",
        "\n",
        "# Save datasets to Parquet format locally\n",
        "!mkdir -p data\n",
        "# Save datasets to Parquet format with overwrite mode\n",
        "categories_df.write.mode('overwrite').parquet(\"data/categories_expanded\")\n",
        "products_df.write.mode('overwrite').parquet(\"data/products_expanded\")\n",
        "customers_df.write.mode('overwrite').parquet(\"data/customers_expanded\")\n",
        "orders_df.write.mode('overwrite').parquet(\"data/orders_expanded\")\n",
        "order_items_df.write.mode('overwrite').parquet(\"data/order_items_expanded\")\n",
        "\n",
        "# Show sample records from each table\n",
        "print(\"\\nSample Categories:\")\n",
        "categories_df.show()\n",
        "\n",
        "print(\"\\nSample Products:\")\n",
        "products_df.show()\n",
        "\n",
        "print(\"\\nSample Customers:\")\n",
        "customers_df.show()\n",
        "\n",
        "print(\"\\nSample Orders:\")\n",
        "orders_df.show()\n",
        "\n",
        "print(\"\\nSample Order Items:\")\n",
        "order_items_df.show()\n",
        "\n",
        "# List saved files\n",
        "print(\"\\nSaved files:\")\n",
        "!ls -lh data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcPqDCLEA1u"
      },
      "source": [
        "### Questionaire:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3WVc-7JEDvt",
        "outputId": "3d41951e-dadd-4e82-8924-c0719caaa42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------------+---------+\n",
            "| id|         name|  country|\n",
            "+---+-------------+---------+\n",
            "|  1|   Jane Doe_1|  Germany|\n",
            "|  2| Bob Wilson_2|Australia|\n",
            "|  3|Alice Brown_3|    India|\n",
            "|  4| Mike Davis_4|      USA|\n",
            "|  5| John Smith_5|       UK|\n",
            "+---+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Create DataFrame with selected columns\n",
        "cust_basic = customers_df.select('id', 'name', 'country')\n",
        "cust_basic.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RO8z-bGE43Z",
        "outputId": "206a88f0-0a6e-420f-c340-b56bfb353da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- join_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. Read Parquet files\n",
        "df_cust = spark.read.parquet(\"data/customers_expanded\")\n",
        "df_cust.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wMEoO8YE5IV",
        "outputId": "ebf8ddc7-ae3b-4336-e974-357dc2c0a8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- base_price: double (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Show schema\n",
        "products_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEowaQreE5LC",
        "outputId": "6e391c4a-92c7-4791-f725-24e09ee521ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+------------+\n",
            "|order_date|customer_id|total_amount|\n",
            "+----------+-----------+------------+\n",
            "|2023-02-08|         20|     2182.79|\n",
            "|2023-02-22|          9|      3053.4|\n",
            "|2023-02-23|         17|      776.12|\n",
            "|2023-03-05|         19|     3047.48|\n",
            "|2023-03-06|         19|     3419.64|\n",
            "|2023-03-13|         15|     3761.67|\n",
            "|2023-03-16|         17|     4959.08|\n",
            "|2023-03-17|         15|     2853.24|\n",
            "|2023-03-21|         13|     3723.45|\n",
            "|2023-03-28|          8|     2499.95|\n",
            "|2023-03-29|         25|     2596.07|\n",
            "|2023-04-11|         21|     2655.56|\n",
            "|2023-04-12|          7|      599.36|\n",
            "|2023-04-12|         19|     1831.24|\n",
            "|2023-04-18|          9|      497.07|\n",
            "|2023-05-03|         22|     3903.87|\n",
            "|2023-05-11|         10|     4033.01|\n",
            "|2023-05-18|         24|     4265.84|\n",
            "|2023-05-27|         22|      418.62|\n",
            "|2023-06-01|          1|     2722.47|\n",
            "+----------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. Select and sort columns\n",
        "from pyspark.sql.functions import asc\n",
        "orders_df.select('order_date', 'customer_id', 'total_amount') \\\n",
        "         .orderBy(asc('order_date')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq2EwuCKE5NV",
        "outputId": "7e735a8a-a0ae-4bc9-ae5b-6002006b81ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------------+----------+\n",
            "| id|         name| join_date|\n",
            "+---+-------------+----------+\n",
            "|  4| Mike Davis_4|2023-08-08|\n",
            "| 17|Bob Wilson_17|2023-10-15|\n",
            "+---+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Filter active US customers\n",
        "from pyspark.sql.functions import year\n",
        "customers_df.filter((col('status') == 'active') &\n",
        "                   (col('country') == 'USA') &\n",
        "                   (year(col('join_date')) == 2023)) \\\n",
        "            .select('id', 'name', 'join_date').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZreBg4_WE5Po",
        "outputId": "141bd8a4-32b1-4d50-e1ac-ffcadc690287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+\n",
            "|product_id|total_sold|\n",
            "+----------+----------+\n",
            "|        19|        16|\n",
            "|        22|         5|\n",
            "|         7|         5|\n",
            "|        25|        10|\n",
            "|         6|        18|\n",
            "|         9|         8|\n",
            "|         1|         9|\n",
            "|        10|        10|\n",
            "|         3|        11|\n",
            "|        12|        16|\n",
            "|         8|        13|\n",
            "|        11|        10|\n",
            "|         2|        21|\n",
            "|         4|         9|\n",
            "|        13|         6|\n",
            "|        18|        17|\n",
            "|        14|        28|\n",
            "|        21|        18|\n",
            "|        15|        11|\n",
            "|        23|        17|\n",
            "+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6. Group by product\n",
        "order_items_df.groupBy('product_id') \\\n",
        "              .agg({'quantity': 'sum'}) \\\n",
        "              .withColumnRenamed('sum(quantity)', 'total_sold').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6OfpbYVE5R5",
        "outputId": "a4592559-5436-4631-91a1-ab7a90975c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+------------+\n",
            "|         name|total_amount|\n",
            "+-------------+------------+\n",
            "|   Jane Doe_1|     2722.47|\n",
            "| Bob Wilson_2|      446.72|\n",
            "| Bob Wilson_2|     4170.33|\n",
            "| Bob Wilson_2|      573.36|\n",
            "| Mike Davis_4|      962.03|\n",
            "| John Smith_5|     3858.49|\n",
            "| John Smith_5|     1754.04|\n",
            "|   Jane Doe_6|      899.91|\n",
            "| Bob Wilson_7|      599.36|\n",
            "|Alice Brown_8|      3641.5|\n",
            "|Alice Brown_8|     2499.95|\n",
            "| Mike Davis_9|      3053.4|\n",
            "| Mike Davis_9|      497.07|\n",
            "|John Smith_10|     4444.31|\n",
            "|John Smith_10|     4033.01|\n",
            "|  Jane Doe_11|      390.74|\n",
            "|  Jane Doe_11|     1017.94|\n",
            "|  Jane Doe_11|     3947.05|\n",
            "|  Jane Doe_11|     1850.26|\n",
            "|Bob Wilson_12|     4052.35|\n",
            "+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 7. Join orders and customers\n",
        "orders_df.join(customers_df, orders_df.customer_id == customers_df.id) \\\n",
        "         .select(customers_df.name, orders_df.total_amount).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uu6gHvoE5Ud",
        "outputId": "96484203-1e0e-4dea-b1ef-f5a90eb70c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+----------+-----------+---------+\n",
            "| id|customer_id|order_date|order_total|   status|\n",
            "+---+-----------+----------+-----------+---------+\n",
            "|  1|         19|2023-04-12|    1831.24|completed|\n",
            "|  2|         21|2023-04-11|    2655.56|  pending|\n",
            "|  3|          5|2023-08-03|    3858.49|cancelled|\n",
            "|  4|         22|2023-05-03|    3903.87|  pending|\n",
            "|  5|         11|2023-09-26|     390.74|  pending|\n",
            "|  6|         19|2023-03-05|    3047.48|completed|\n",
            "|  7|          6|2023-12-07|     899.91|completed|\n",
            "|  8|          4|2023-06-25|     962.03|cancelled|\n",
            "|  9|          2|2023-06-02|     446.72|  pending|\n",
            "| 10|          9|2023-02-22|     3053.4|  pending|\n",
            "| 11|          2|2023-11-29|    4170.33|cancelled|\n",
            "| 12|         22|2023-11-13|    2007.06|completed|\n",
            "| 13|         15|2023-03-17|    2853.24|  pending|\n",
            "| 14|          1|2023-06-01|    2722.47|completed|\n",
            "| 15|         24|2023-05-18|    4265.84|completed|\n",
            "| 16|         17|2023-12-27|     2634.2|  pending|\n",
            "| 17|         19|2023-03-06|    3419.64|completed|\n",
            "| 18|         12|2023-10-10|    4052.35|completed|\n",
            "| 19|         20|2023-09-22|    3619.82|  pending|\n",
            "| 20|         22|2023-05-27|     418.62|cancelled|\n",
            "+---+-----------+----------+-----------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 8. Rename column\n",
        "orders_renamed = orders_df.withColumnRenamed('total_amount', 'order_total')\n",
        "orders_renamed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27bYDNbE5Wt",
        "outputId": "f4a340fa-39c4-4425-f79d-079b770e0a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-----------+----------+--------+\n",
            "| id|      name|category_id|base_price|  status|\n",
            "+---+----------+-----------+----------+--------+\n",
            "|  1|Smartphone|          1|    199.99|  active|\n",
            "|  2|    Laptop|          1|    599.99|  active|\n",
            "|  3|Headphones|          1|    199.99|  active|\n",
            "|  4|    Tablet|          1|    149.99|  active|\n",
            "|  5|Smartwatch|          1|    199.99|  active|\n",
            "|  6|Power Bank|          1|    299.99|inactive|\n",
            "|  7|   T-Shirt|          2|     19.99|inactive|\n",
            "|  8|     Jeans|          2|     79.99|  active|\n",
            "|  9|     Shoes|          2|     79.99|  active|\n",
            "| 10|    Jacket|          2|     29.99|  active|\n",
            "| 11|       Hat|          2|     79.99|  active|\n",
            "| 12|     Scarf|          2|     19.99|  active|\n",
            "| 13|      Sofa|          3|    799.99|  active|\n",
            "| 14|       Bed|          3|    599.99|inactive|\n",
            "| 15|     Chair|          3|    499.99|  active|\n",
            "| 16|     Table|          3|    599.99|  active|\n",
            "| 17|      Desk|          3|    799.99|  active|\n",
            "| 18| Bookshelf|          3|    799.99|  active|\n",
            "| 19|Basketball|          4|     79.99|  active|\n",
            "| 20|  Football|          4|    299.99|  active|\n",
            "+---+----------+-----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 9. Handle nulls\n",
        "products_clean = products_df.fillna({\n",
        "    'base_price': 0,\n",
        "    'status': 'unknown',\n",
        "    'name': 'unknown_product'\n",
        "})\n",
        "products_clean.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZn9YtuSE5Zj"
      },
      "outputs": [],
      "source": [
        "# 10. Add order year\n",
        "from pyspark.sql.functions import year\n",
        "orders_with_year = orders_df.withColumn('order_year', year('order_date'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3H5T35CE5cM",
        "outputId": "c02f8500-285c-4f8f-8a2e-3b6fdfc999cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+----------+------------+---------+------------------+\n",
            "| id|customer_id|order_date|total_amount|   status|     running_total|\n",
            "+---+-----------+----------+------------+---------+------------------+\n",
            "| 14|          1|2023-06-01|     2722.47|completed|           2722.47|\n",
            "|  9|          2|2023-06-02|      446.72|  pending|            446.72|\n",
            "| 49|          2|2023-11-27|      573.36|cancelled|           1020.08|\n",
            "| 11|          2|2023-11-29|     4170.33|cancelled|           5190.41|\n",
            "|  8|          4|2023-06-25|      962.03|cancelled|            962.03|\n",
            "|  3|          5|2023-08-03|     3858.49|cancelled|           3858.49|\n",
            "| 39|          5|2023-08-04|     1754.04|completed|           5612.53|\n",
            "|  7|          6|2023-12-07|      899.91|completed|            899.91|\n",
            "| 47|          7|2023-04-12|      599.36|completed|            599.36|\n",
            "| 50|          8|2023-03-28|     2499.95|  pending|           2499.95|\n",
            "| 36|          8|2023-10-11|      3641.5|completed|           6141.45|\n",
            "| 10|          9|2023-02-22|      3053.4|  pending|            3053.4|\n",
            "| 43|          9|2023-04-18|      497.07|cancelled|3550.4700000000003|\n",
            "| 35|         10|2023-05-11|     4033.01|cancelled|           4033.01|\n",
            "| 34|         10|2023-07-19|     4444.31|cancelled|           8477.32|\n",
            "| 30|         11|2023-08-15|     1017.94|  pending|           1017.94|\n",
            "|  5|         11|2023-09-26|      390.74|  pending|           1408.68|\n",
            "| 38|         11|2023-10-08|     3947.05|  pending|5355.7300000000005|\n",
            "| 46|         11|2023-11-01|     1850.26|  pending| 7205.990000000001|\n",
            "| 26|         12|2023-06-13|     2497.72|  pending|           2497.72|\n",
            "+---+-----------+----------+------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 11. Window functions\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "windowSpec = Window.partitionBy('customer_id').orderBy('order_date').rowsBetween(Window.unboundedPreceding, 0)\n",
        "orders_df.withColumn('running_total', _sum('total_amount').over(windowSpec)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9RW6mk2E5eN",
        "outputId": "60806769-90d8-4aee-8825-afd49c5a2944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------------+-----------+\n",
            "|month|       total_sales|order_count|\n",
            "+-----+------------------+-----------+\n",
            "|   12|3534.1099999999997|          2|\n",
            "|    6| 7705.679999999999|          5|\n",
            "|    3|26860.579999999998|          8|\n",
            "|    5|          12621.34|          4|\n",
            "|    9|5635.4800000000005|          4|\n",
            "|    4|5583.2300000000005|          4|\n",
            "|    8|11410.579999999998|          5|\n",
            "|    7|          12671.45|          4|\n",
            "|   10|14868.960000000001|          5|\n",
            "|   11|12212.329999999998|          6|\n",
            "|    2|6012.3099999999995|          3|\n",
            "+-----+------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 12. Monthly stats\n",
        "from pyspark.sql.functions import month,count\n",
        "orders_df.groupBy(month('order_date').alias('month')) \\\n",
        "         .agg(_sum('total_amount').alias('total_sales'),\n",
        "              count('*').alias('order_count')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QXe3OAuE5hr",
        "outputId": "17b00fcc-92d3-4e2a-8258-496ffb8c1010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+--------------------+\n",
            "|       name|product_count|       product_names|\n",
            "+-----------+-------------+--------------------+\n",
            "|Electronics|            6|[Smartphone, Lapt...|\n",
            "| Home Goods|            6|[Sofa, Bed, Chair...|\n",
            "|   Clothing|            6|[T-Shirt, Jeans, ...|\n",
            "|     Sports|            7|[Basketball, Foot...|\n",
            "+-----------+-------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 13. Category-product counts\n",
        "from pyspark.sql.functions import collect_list\n",
        "(products_df.groupBy('category_id')\n",
        "            .agg(count('*').alias('product_count'),collect_list('name').alias('product_names'))\n",
        "            .withColumnRenamed('category_id', 'id')\n",
        "            .join(categories_df, 'id', \"inner\")\n",
        "            .select('name', 'product_count','product_names')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du4gIEZyMNCr",
        "outputId": "8590d85c-21fe-42dd-e83f-2c1a5a68cb1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+----------+------------+---------+\n",
            "| id|customer_id|order_date|total_amount|   status|\n",
            "+---+-----------+----------+------------+---------+\n",
            "|  1|         19|2023-04-12|     1831.24|completed|\n",
            "|  2|         21|2023-04-11|     2655.56|  pending|\n",
            "|  3|          5|2023-08-03|     3858.49|cancelled|\n",
            "|  4|         22|2023-05-03|     3903.87|  pending|\n",
            "|  5|         11|2023-09-26|      390.74|  pending|\n",
            "|  6|         19|2023-03-05|     3047.48|completed|\n",
            "|  7|          6|2023-12-07|      899.91|completed|\n",
            "|  8|          4|2023-06-25|      962.03|cancelled|\n",
            "|  9|          2|2023-06-02|      446.72|  pending|\n",
            "| 10|          9|2023-02-22|      3053.4|  pending|\n",
            "| 11|          2|2023-11-29|     4170.33|cancelled|\n",
            "| 12|         22|2023-11-13|     2007.06|completed|\n",
            "| 13|         15|2023-03-17|     2853.24|  pending|\n",
            "| 14|          1|2023-06-01|     2722.47|completed|\n",
            "| 15|         24|2023-05-18|     4265.84|completed|\n",
            "| 16|         17|2023-12-27|      2634.2|  pending|\n",
            "| 17|         19|2023-03-06|     3419.64|completed|\n",
            "| 18|         12|2023-10-10|     4052.35|completed|\n",
            "| 19|         20|2023-09-22|     3619.82|  pending|\n",
            "| 20|         22|2023-05-27|      418.62|cancelled|\n",
            "+---+-----------+----------+------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---+--------+----------+--------+------+\n",
            "| id|order_id|product_id|quantity| price|\n",
            "+---+--------+----------+--------+------+\n",
            "|  1|      30|        21|       3|299.99|\n",
            "|  2|      37|        10|       3| 29.99|\n",
            "|  3|      39|        23|       5|129.99|\n",
            "|  4|      35|         2|       5|599.99|\n",
            "|  5|      31|        14|       5|599.99|\n",
            "|  6|      25|        16|       3|599.99|\n",
            "|  7|       2|         3|       3|199.99|\n",
            "|  8|      13|        23|       3|129.99|\n",
            "|  9|      47|        25|       5|299.99|\n",
            "| 10|      33|         6|       4|299.99|\n",
            "| 11|      25|        14|       3|599.99|\n",
            "| 12|      36|        23|       2|129.99|\n",
            "| 13|      34|        14|       3|599.99|\n",
            "| 14|      30|         1|       2|199.99|\n",
            "| 15|      40|        13|       1|799.99|\n",
            "| 16|      36|        14|       5|599.99|\n",
            "| 17|      24|        14|       4|599.99|\n",
            "| 18|      48|         6|       4|299.99|\n",
            "| 19|       3|        19|       4| 79.99|\n",
            "| 20|      22|         9|       2| 79.99|\n",
            "+---+--------+----------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---+----------+-----------+----------+--------+\n",
            "| id|      name|category_id|base_price|  status|\n",
            "+---+----------+-----------+----------+--------+\n",
            "|  1|Smartphone|          1|    199.99|  active|\n",
            "|  2|    Laptop|          1|    599.99|  active|\n",
            "|  3|Headphones|          1|    199.99|  active|\n",
            "|  4|    Tablet|          1|    149.99|  active|\n",
            "|  5|Smartwatch|          1|    199.99|  active|\n",
            "|  6|Power Bank|          1|    299.99|inactive|\n",
            "|  7|   T-Shirt|          2|     19.99|inactive|\n",
            "|  8|     Jeans|          2|     79.99|  active|\n",
            "|  9|     Shoes|          2|     79.99|  active|\n",
            "| 10|    Jacket|          2|     29.99|  active|\n",
            "| 11|       Hat|          2|     79.99|  active|\n",
            "| 12|     Scarf|          2|     19.99|  active|\n",
            "| 13|      Sofa|          3|    799.99|  active|\n",
            "| 14|       Bed|          3|    599.99|inactive|\n",
            "| 15|     Chair|          3|    499.99|  active|\n",
            "| 16|     Table|          3|    599.99|  active|\n",
            "| 17|      Desk|          3|    799.99|  active|\n",
            "| 18| Bookshelf|          3|    799.99|  active|\n",
            "| 19|Basketball|          4|     79.99|  active|\n",
            "| 20|  Football|          4|    299.99|  active|\n",
            "+---+----------+-----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "orders_df.show()\n",
        "order_items_df.show()\n",
        "products_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7THU972dGVLF",
        "outputId": "ba8af060-f4ba-44be-f688-a013f8a82254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+----------+------------+---------+-----------+\n",
            "| id|customer_id|order_date|total_amount|   status|category_id|\n",
            "+---+-----------+----------+------------+---------+-----------+\n",
            "| 18|         12|2023-10-10|     4052.35|completed|          2|\n",
            "| 45|         19|2023-08-28|      997.64|cancelled|          2|\n",
            "| 30|         11|2023-08-15|     1017.94|  pending|          1|\n",
            "| 34|         10|2023-07-19|     4444.31|cancelled|          1|\n",
            "|  3|          5|2023-08-03|     3858.49|cancelled|          1|\n",
            "| 48|         17|2023-03-16|     4959.08|cancelled|          1|\n",
            "| 33|         20|2023-11-07|     2833.69|  pending|          1|\n",
            "|  9|          2|2023-06-02|      446.72|  pending|          1|\n",
            "| 48|         17|2023-03-16|     4959.08|cancelled|          2|\n",
            "|  5|         11|2023-09-26|      390.74|  pending|          2|\n",
            "| 22|         25|2023-03-29|     2596.07|completed|          2|\n",
            "| 23|         17|2023-08-02|     3782.47|cancelled|          1|\n",
            "| 44|         13|2023-03-21|     3723.45|  pending|          1|\n",
            "| 23|         17|2023-08-02|     3782.47|cancelled|          1|\n",
            "| 44|         13|2023-03-21|     3723.45|  pending|          1|\n",
            "| 30|         11|2023-08-15|     1017.94|  pending|          1|\n",
            "|  8|          4|2023-06-25|      962.03|cancelled|          2|\n",
            "|  7|          6|2023-12-07|      899.91|completed|          2|\n",
            "| 37|         25|2023-11-27|      777.63|cancelled|          2|\n",
            "| 24|         13|2023-07-06|     3628.43|completed|          1|\n",
            "+---+-----------+----------+------------+---------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 14. Pivot table\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Join orders, order_items, and products to link orders to categories\n",
        "orders_with_categories = orders_df.alias(\"o\") \\\n",
        "    .join(order_items_df.alias(\"oi\"), col(\"o.id\") == col(\"oi.order_id\")) \\\n",
        "    .join(products_df.alias(\"p\"), col(\"oi.product_id\") == col(\"p.id\")) \\\n",
        "    .select(\"o.*\", \"p.category_id\")\n",
        "\n",
        "orders_with_categories.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q6Hqv61F3kQ",
        "outputId": "9bd99b2a-e9dc-4a5d-b03b-0c36f654f0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+---------+---------+-------+\n",
            "|c.country_o.status|cancelled|completed|pending|\n",
            "+------------------+---------+---------+-------+\n",
            "|           Germany|        0|        1|      0|\n",
            "|            France|        1|        4|      4|\n",
            "|             India|        2|        2|      1|\n",
            "|               USA|        3|        1|      3|\n",
            "|                UK|        1|        2|      0|\n",
            "|            Canada|        0|        1|      5|\n",
            "|             Japan|        5|        5|      4|\n",
            "|         Australia|        3|        0|      2|\n",
            "+------------------+---------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 15. Cross-tab\n",
        "orders_with_customers = orders_df.alias(\"o\") \\\n",
        "    .join(customers_df.alias(\"c\"), col(\"o.customer_id\") == col(\"c.id\"))\n",
        "orders_with_customers.stat.crosstab('c.country', 'o.status').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZiXh8djOSa-",
        "outputId": "bebf58f9-3ad5-43f4-c493-e3a919db6b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+----------+------------+---------+---+--------+----------+--------+------+\n",
            "| id|customer_id|order_date|total_amount|   status| id|order_id|product_id|quantity| price|\n",
            "+---+-----------+----------+------------+---------+---+--------+----------+--------+------+\n",
            "| 29|         20|2023-02-08|     2182.79|cancelled| 52|      29|        23|       3|129.99|\n",
            "| 29|         20|2023-02-08|     2182.79|cancelled| 89|      29|         4|       2|149.99|\n",
            "| 19|         20|2023-09-22|     3619.82|  pending| 72|      19|         2|       2|599.99|\n",
            "| 34|         10|2023-07-19|     4444.31|cancelled| 13|      34|        14|       3|599.99|\n",
            "| 34|         10|2023-07-19|     4444.31|cancelled| 77|      34|        23|       1|129.99|\n",
            "| 34|         10|2023-07-19|     4444.31|cancelled| 88|      34|         6|       3|299.99|\n",
            "| 31|         22|2023-07-14|     4218.89|cancelled|  5|      31|        14|       5|599.99|\n",
            "| 31|         22|2023-07-14|     4218.89|cancelled| 86|      31|        16|       1|599.99|\n",
            "| 25|         15|2023-03-13|     3761.67|  pending|  6|      25|        16|       3|599.99|\n",
            "| 25|         15|2023-03-13|     3761.67|  pending| 11|      25|        14|       3|599.99|\n",
            "| 25|         15|2023-03-13|     3761.67|  pending| 61|      25|        19|       5| 79.99|\n",
            "| 25|         15|2023-03-13|     3761.67|  pending| 90|      25|        16|       3|599.99|\n",
            "| 33|         20|2023-11-07|     2833.69|  pending| 10|      33|         6|       4|299.99|\n",
            "| 33|         20|2023-11-07|     2833.69|  pending| 51|      33|        19|       2| 79.99|\n",
            "| 28|         21|2023-10-27|      427.15|  pending| 87|      28|         4|       4|149.99|\n",
            "| 28|         21|2023-10-27|      427.15|  pending| 91|      28|        17|       4|799.99|\n",
            "|  5|         11|2023-09-26|      390.74|  pending| 33|       5|         9|       2| 79.99|\n",
            "|  5|         11|2023-09-26|      390.74|  pending| 92|       5|        15|       4|499.99|\n",
            "| 48|         17|2023-03-16|     4959.08|cancelled| 18|      48|         6|       4|299.99|\n",
            "| 48|         17|2023-03-16|     4959.08|cancelled| 23|      48|        21|       4|299.99|\n",
            "+---+-----------+----------+------------+---------+---+--------+----------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#16. Optimize memory usage when joining orders with order_items tables for large datasets.\n",
        "orders_df.repartition(4, \"customer_id\").createOrReplaceTempView(\"orders\")\n",
        "order_items_df.repartition(4, \"order_id\").createOrReplaceTempView(\"order_items\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT /*+ COALESCE(4) */ o.*, oi.*\n",
        "    FROM orders o\n",
        "    JOIN order_items oi ON o.id = oi.order_id\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvntttbzITEL",
        "outputId": "f19fa047-4d7b-4605-c3c0-cf4577820f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-----------+----------+--------+----+---+--------+----------+--------+------+\n",
            "| id|      name|category_id|base_price|  status|salt| id|order_id|product_id|quantity| price|\n",
            "+---+----------+-----------+----------+--------+----+---+--------+----------+--------+------+\n",
            "|  9|     Shoes|          2|     79.99|  active|   8| 34|      48|         9|       4| 79.99|\n",
            "|  8|     Jeans|          2|     79.99|  active|   8| 49|      38|         8|       3| 79.99|\n",
            "| 12|     Scarf|          2|     19.99|  active|   9| 22|      39|        12|       5| 19.99|\n",
            "|  2|    Laptop|          1|    599.99|  active|   0| 99|      40|         2|       1|599.99|\n",
            "|  2|    Laptop|          1|    599.99|  active|   0| 82|      30|         2|       3|599.99|\n",
            "|  2|    Laptop|          1|    599.99|  active|   0| 44|      20|         2|       3|599.99|\n",
            "| 25|    Helmet|          4|    299.99|  active|   7| 70|      47|        25|       1|299.99|\n",
            "| 25|    Helmet|          4|    299.99|  active|   7|  9|      47|        25|       5|299.99|\n",
            "| 19|Basketball|          4|     79.99|  active|   5| 61|      25|        19|       5| 79.99|\n",
            "| 13|      Sofa|          3|    799.99|  active|   0| 15|      40|        13|       1|799.99|\n",
            "| 23|Golf Clubs|          4|    129.99|  active|   4| 77|      34|        23|       1|129.99|\n",
            "| 15|     Chair|          3|    499.99|  active|   7| 31|      27|        15|       3|499.99|\n",
            "| 16|     Table|          3|    599.99|  active|   9| 65|      49|        16|       5|599.99|\n",
            "| 20|  Football|          4|    299.99|  active|   2| 53|      32|        20|       4|299.99|\n",
            "| 20|  Football|          4|    299.99|  active|   2| 46|       2|        20|       4|299.99|\n",
            "| 14|       Bed|          3|    599.99|inactive|   6| 16|      36|        14|       5|599.99|\n",
            "| 17|      Desk|          3|    799.99|  active|   7| 85|      47|        17|       1|799.99|\n",
            "+---+----------+-----------+----------+--------+----+---+--------+----------+--------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#17. Handle skewed data distribution in the products table by implementing salting on the category_id column.\n",
        "from pyspark.sql.functions import rand, concat, lit\n",
        "salted_products_df = products_df.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
        "salted_products_df.createOrReplaceTempView(\"salted_products\")\n",
        "order_items_df.createOrReplaceTempView(\"order_items\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT sp.*, oi.*\n",
        "    FROM salted_products sp\n",
        "    JOIN order_items oi ON sp.id = oi.product_id AND sp.salt = (oi.order_id % 10)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-cgcbc8Of7C",
        "outputId": "e3bd2529-10b3-42f5-ee0a-529b356748be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE) = 1603.5202079422497\n"
          ]
        }
      ],
      "source": [
        "#18. Use MLlib to predict future sales based on historical order data.\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql.functions import to_date, dayofweek, weekofyear, month, year\n",
        "\n",
        "# Convert date to numerical features\n",
        "orders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
        "    .withColumn(\"dayofweek\", dayofweek(col(\"order_date\"))) \\\n",
        "    .withColumn(\"weekofyear\", weekofyear(col(\"order_date\"))) \\\n",
        "    .withColumn(\"month\", month(col(\"order_date\"))) \\\n",
        "    .withColumn(\"year\", year(col(\"order_date\")))\n",
        "assembler = VectorAssembler(inputCols=[\"customer_id\", \"dayofweek\", \"weekofyear\", \"month\", \"year\"], outputCol=\"features\")\n",
        "assembled_orders_df = assembler.transform(orders_df)\n",
        "(training_data, test_data) = assembled_orders_df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
        "model = lr.fit(training_data)\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) = \" + str(rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Iq4J63uOlgS",
        "outputId": "21f5a36a-fd92-4b14-f2df-d6982574710f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+----------+------------+---------+---------+----------+-----+----+---+--------+----------+--------+------+---+-------------+-----------+----------+--------+\n",
            "| id|customer_id|order_date|total_amount|   status|dayofweek|weekofyear|month|year| id|order_id|product_id|quantity| price| id|         name|category_id|base_price|  status|\n",
            "+---+-----------+----------+------------+---------+---------+----------+-----+----+---+--------+----------+--------+------+---+-------------+-----------+----------+--------+\n",
            "| 22|         25|2023-03-29|     2596.07|completed|        4|        13|    3|2023| 20|      22|         9|       2| 79.99|  9|        Shoes|          2|     79.99|  active|\n",
            "| 34|         10|2023-07-19|     4444.31|cancelled|        4|        29|    7|2023| 13|      34|        14|       3|599.99| 14|          Bed|          3|    599.99|inactive|\n",
            "| 32|         17|2023-02-23|      776.12|completed|        5|         8|    2|2023| 27|      32|         8|       5| 79.99|  8|        Jeans|          2|     79.99|  active|\n",
            "| 43|          9|2023-04-18|      497.07|cancelled|        3|        16|    4|2023| 28|      43|        12|       2| 19.99| 12|        Scarf|          2|     19.99|  active|\n",
            "| 32|         17|2023-02-23|      776.12|completed|        5|         8|    2|2023| 50|      32|        19|       1| 79.99| 19|   Basketball|          4|     79.99|  active|\n",
            "| 31|         22|2023-07-14|     4218.89|cancelled|        6|        28|    7|2023|  5|      31|        14|       5|599.99| 14|          Bed|          3|    599.99|inactive|\n",
            "| 39|          5|2023-08-04|     1754.04|completed|        6|        31|    8|2023|  3|      39|        23|       5|129.99| 23|   Golf Clubs|          4|    129.99|  active|\n",
            "| 39|          5|2023-08-04|     1754.04|completed|        6|        31|    8|2023| 21|      39|        11|       2| 79.99| 11|          Hat|          2|     79.99|  active|\n",
            "| 39|          5|2023-08-04|     1754.04|completed|        6|        31|    8|2023| 22|      39|        12|       5| 19.99| 12|        Scarf|          2|     19.99|  active|\n",
            "| 25|         15|2023-03-13|     3761.67|  pending|        2|        11|    3|2023|  6|      25|        16|       3|599.99| 16|        Table|          3|    599.99|  active|\n",
            "| 25|         15|2023-03-13|     3761.67|  pending|        2|        11|    3|2023| 11|      25|        14|       3|599.99| 14|          Bed|          3|    599.99|inactive|\n",
            "|  9|          2|2023-06-02|      446.72|  pending|        6|        22|    6|2023| 36|       9|         6|       1|299.99|  6|   Power Bank|          1|    299.99|inactive|\n",
            "|  9|          2|2023-06-02|      446.72|  pending|        6|        22|    6|2023| 47|       9|         4|       3|149.99|  4|       Tablet|          1|    149.99|  active|\n",
            "| 27|         13|2023-07-05|      379.82|completed|        4|        27|    7|2023| 31|      27|        15|       3|499.99| 15|        Chair|          3|    499.99|  active|\n",
            "| 33|         20|2023-11-07|     2833.69|  pending|        3|        45|   11|2023| 10|      33|         6|       4|299.99|  6|   Power Bank|          1|    299.99|inactive|\n",
            "|  5|         11|2023-09-26|      390.74|  pending|        3|        39|    9|2023| 33|       5|         9|       2| 79.99|  9|        Shoes|          2|     79.99|  active|\n",
            "|  1|         19|2023-04-12|     1831.24|completed|        4|        15|    4|2023| 29|       1|        11|       1| 79.99| 11|          Hat|          2|     79.99|  active|\n",
            "| 10|          9|2023-02-22|      3053.4|  pending|        4|         8|    2|2023| 38|      10|        22|       1| 99.99| 22|Running Shoes|          4|     99.99|  active|\n",
            "| 10|          9|2023-02-22|      3053.4|  pending|        4|         8|    2|2023| 39|      10|        24|       1|299.99| 24|       Skates|          4|    299.99|  active|\n",
            "| 10|          9|2023-02-22|      3053.4|  pending|        4|         8|    2|2023| 45|      10|        19|       4| 79.99| 19|   Basketball|          4|     79.99|  active|\n",
            "+---+-----------+----------+------------+---------+---------+----------+-----+----+---+--------+----------+--------+------+---+-------------+-----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#19. Optimize joins between orders, order_items, and products tables using broadcast hints.\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "orders_df.createOrReplaceTempView(\"orders\")\n",
        "order_items_df.createOrReplaceTempView(\"order_items\")\n",
        "products_df.createOrReplaceTempView(\"products\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT /*+ BROADCAST(p) */ o.*, oi.*, p.*\n",
        "    FROM orders o\n",
        "    JOIN order_items oi ON o.id = oi.order_id\n",
        "    JOIN products p ON oi.product_id = p.id\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1mV00ZSOo8-",
        "outputId": "e19f45f4-98a3-4a70-e45c-19e2b53f7859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- join_date: date (nullable = true)\n",
            "\n",
            "+---+--------------+---------+--------+----------+\n",
            "| id|          name|  country|  status| join_date|\n",
            "+---+--------------+---------+--------+----------+\n",
            "| 13|Alice Brown_13|   France|  active|2023-11-20|\n",
            "| 14| Mike Davis_14|       UK|  active|2023-08-30|\n",
            "| 15| John Smith_15|   France|inactive|2023-09-16|\n",
            "| 16|   Jane Doe_16|   Canada|  active|2023-07-07|\n",
            "| 17| Bob Wilson_17|      USA|  active|2023-10-15|\n",
            "| 18|Alice Brown_18|    India|  active|2023-06-27|\n",
            "| 19| Mike Davis_19|    Japan|  active|2023-06-10|\n",
            "| 20| John Smith_20|    Japan|  active|2023-04-12|\n",
            "| 21|   Jane Doe_21|    Japan|  active|2023-01-15|\n",
            "| 22| Bob Wilson_22|    India|  active|2023-12-24|\n",
            "| 23|Alice Brown_23|    Japan|  active|2023-10-16|\n",
            "| 24| Mike Davis_24|       UK|  active|2023-03-12|\n",
            "| 25| John Smith_25|   France|  active|2023-04-02|\n",
            "|  1|    Jane Doe_1|  Germany|  active|2023-02-13|\n",
            "|  2|  Bob Wilson_2|Australia|inactive|2023-12-08|\n",
            "|  3| Alice Brown_3|    India|  active|2023-12-28|\n",
            "|  4|  Mike Davis_4|      USA|  active|2023-08-08|\n",
            "|  5|  John Smith_5|       UK|  active|2023-09-07|\n",
            "|  6|    Jane Doe_6|    India|  active|2023-10-02|\n",
            "|  7|  Bob Wilson_7|    Japan|  active|2023-10-23|\n",
            "+---+--------------+---------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#20. Convert the customers table between Parquet and CSV formats while maintaining schema integrity.\n",
        "customers_df.write.csv(\"data/customers.csv\", mode=\"overwrite\", header=True)\n",
        "\n",
        "csv_customers_df = spark.read.csv(\"data/customers.csv\", header=True, inferSchema=True)\n",
        "csv_customers_df.printSchema()\n",
        "csv_customers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liKu8G7uOxRh",
        "outputId": "58edf21a-e6d3-4a93-9918-9e0281398b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Area Under ROC (AUC) = 0.0\n"
          ]
        }
      ],
      "source": [
        "#21. Build a classification model to predict high-value customers (> $5000 total spending) using features from the orders table.\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Define a high-value customer as one with total spending > $5000\n",
        "orders_with_high_value = orders_df.withColumn(\"high_value\", when(col(\"total_amount\") > 5000, 1).otherwise(0))\n",
        "\n",
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(inputCols=[\"customer_id\", \"total_amount\"], outputCol=\"features\")\n",
        "assembled_orders_df = assembler.transform(orders_with_high_value)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "(training_data, test_data) = assembled_orders_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"high_value\")\n",
        "model = lr.fit(training_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"high_value\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(\"Area Under ROC (AUC) = \" + str(auc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V61Kas6OPDFA",
        "outputId": "76c91458-7e59-40a0-8222-18e83060f7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+------------+---------+---------+----------+-----+----+----------+\n",
            "| id|customer_id|total_amount|   status|dayofweek|weekofyear|month|year|order_date|\n",
            "+---+-----------+------------+---------+---------+----------+-----+----+----------+\n",
            "| 40|         20|     1076.74|cancelled|        5|        26|    6|2023|2023-06-29|\n",
            "| 14|          1|     2722.47|completed|        5|        22|    6|2023|2023-06-01|\n",
            "|  8|          4|      962.03|cancelled|        1|        25|    6|2023|2023-06-25|\n",
            "|  9|          2|      446.72|  pending|        6|        22|    6|2023|2023-06-02|\n",
            "| 26|         12|     2497.72|  pending|        3|        24|    6|2023|2023-06-13|\n",
            "+---+-----------+------------+---------+---------+----------+-----+----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#22. Implement partition pruning on the orders table using order_date for faster date-range queries.\n",
        "orders_df.write.partitionBy(\"order_date\").parquet(\"data/partitioned_orders\")\n",
        "\n",
        "partitioned_orders_df = spark.read.parquet(\"data/partitioned_orders\")\n",
        "partitioned_orders_df.createOrReplaceTempView(\"partitioned_orders\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM partitioned_orders\n",
        "    WHERE order_date BETWEEN '2023-06-01' AND '2023-06-30'\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KsyPx0YPHjy",
        "outputId": "24bdd310-a3b3-49b3-a4bd-3f9bcf91961c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------+------------------+\n",
            "|order_id|total_quantity|       total_price|\n",
            "+--------+--------------+------------------+\n",
            "|      39|            18|           1409.94|\n",
            "|      33|             6|            379.98|\n",
            "|      28|             8|            949.98|\n",
            "|       1|             1|             79.99|\n",
            "|      48|            19|           1779.95|\n",
            "|       2|             7|            499.98|\n",
            "|      30|            10|           1399.96|\n",
            "|      40|             8|           2499.96|\n",
            "|      45|            10|469.96000000000004|\n",
            "|      29|             5|            279.98|\n",
            "|      25|            14|           1879.96|\n",
            "|       9|             4|            449.98|\n",
            "|      10|             6|            479.97|\n",
            "|      37|             6|            629.98|\n",
            "|       4|             2|           1099.98|\n",
            "|      15|            12|           1519.96|\n",
            "|      23|            18|           1579.95|\n",
            "|      16|            12|            749.97|\n",
            "|       5|             6|            579.98|\n",
            "|      36|            10|            929.97|\n",
            "+--------+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#23. Handle memory optimization when processing large order_item datasets with multiple aggregations.\n",
        "#Reduce the number of partitions before aggregation\n",
        "repartitioned_order_items_df = order_items_df.repartition(10)\n",
        "\n",
        "# Perform aggregations\n",
        "aggregated_order_items = repartitioned_order_items_df.groupBy(\"order_id\") \\\n",
        "    .agg(_sum(\"quantity\").alias(\"total_quantity\"), _sum(\"price\").alias(\"total_price\"))\n",
        "\n",
        "aggregated_order_items.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thtgtSiCPIaZ",
        "outputId": "68577240-2b4b-407b-f7b7-f9043f26e39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+----------+--------+------+-------------+\n",
            "| id|order_id|product_id|quantity| price|valid_product|\n",
            "+---+--------+----------+--------+------+-------------+\n",
            "|  1|      30|        21|       3|299.99|            1|\n",
            "|  2|      37|        10|       3| 29.99|            1|\n",
            "|  3|      39|        23|       5|129.99|            1|\n",
            "|  4|      35|         2|       5|599.99|            1|\n",
            "|  5|      31|        14|       5|599.99|            1|\n",
            "|  6|      25|        16|       3|599.99|            1|\n",
            "|  7|       2|         3|       3|199.99|            1|\n",
            "|  8|      13|        23|       3|129.99|            1|\n",
            "|  9|      47|        25|       5|299.99|            1|\n",
            "| 10|      33|         6|       4|299.99|            1|\n",
            "| 11|      25|        14|       3|599.99|            1|\n",
            "| 12|      36|        23|       2|129.99|            1|\n",
            "| 13|      34|        14|       3|599.99|            1|\n",
            "| 14|      30|         1|       2|199.99|            1|\n",
            "| 15|      40|        13|       1|799.99|            1|\n",
            "| 16|      36|        14|       5|599.99|            1|\n",
            "| 17|      24|        14|       4|599.99|            1|\n",
            "| 18|      48|         6|       4|299.99|            1|\n",
            "| 19|       3|        19|       4| 79.99|            1|\n",
            "| 20|      22|         9|       2| 79.99|            1|\n",
            "+---+--------+----------+--------+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#24. Implement robust error handling for missing product IDs in the order_items table.\n",
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "# Add a check for missing product IDs\n",
        "valid_product_ids = [row.id for row in products_df.select('id').collect()]\n",
        "\n",
        "order_items_checked_df = order_items_df.withColumn(\"valid_product\", when(col(\"product_id\").isin(valid_product_ids), 1).otherwise(0))\n",
        "\n",
        "order_items_checked_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrBJcAs0PPtZ",
        "outputId": "fcf1e77d-b631-4857-c4c5-9d1f3bf300e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of orphaned order items:  0\n"
          ]
        }
      ],
      "source": [
        "# 25. Validate data integrity across related tables (orders → order_items → products).\n",
        "# Check for orphaned order items\n",
        "order_items_with_orders = order_items_df.join(orders_df, order_items_df.order_id == orders_df.id, \"left_outer\")\n",
        "\n",
        "orphaned_order_items = order_items_with_orders.filter(orders_df.id.isNull())\n",
        "\n",
        "print(\"Number of orphaned order items: \", orphaned_order_items.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzUKp21JPRL7"
      },
      "outputs": [],
      "source": [
        "#26. Optimize shuffle operations during aggregation of large-scale order data.\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVDIPT7HPMGd",
        "outputId": "3f284104-c752-420f-f117-f3daa603583a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------------+------------------+------------------+\n",
            "|category_id|              month|       total_sales|       rolling_avg|\n",
            "+-----------+-------------------+------------------+------------------+\n",
            "|          1|2023-02-01 00:00:00|           2182.79|           2182.79|\n",
            "|          1|2023-03-01 00:00:00|          12405.98|          7294.385|\n",
            "|          1|2023-04-01 00:00:00|           2655.56| 5748.110000000001|\n",
            "|          1|2023-05-01 00:00:00| 8717.470000000001| 7926.336666666667|\n",
            "|          1|2023-06-01 00:00:00|           1970.18| 4447.736666666667|\n",
            "|          1|2023-07-01 00:00:00|           8072.74| 6253.463333333333|\n",
            "|          1|2023-08-01 00:00:00|          14477.25| 8173.389999999999|\n",
            "|          1|2023-09-01 00:00:00|           3619.82| 8723.269999999999|\n",
            "|          1|2023-10-01 00:00:00|           4068.65| 7388.573333333334|\n",
            "|          1|2023-11-01 00:00:00|           2833.69|3507.3866666666668|\n",
            "|          1|2023-12-01 00:00:00|            2634.2| 3178.846666666667|\n",
            "|          2|2023-02-01 00:00:00|            776.12|            776.12|\n",
            "|          2|2023-03-01 00:00:00|           7555.15|          4165.635|\n",
            "|          2|2023-04-01 00:00:00|           3527.03| 3952.766666666667|\n",
            "|          2|2023-05-01 00:00:00|           4265.84| 5116.006666666667|\n",
            "|          2|2023-06-01 00:00:00|            962.03|2918.3000000000006|\n",
            "|          2|2023-08-01 00:00:00|14898.359999999999| 6708.743333333333|\n",
            "|          2|2023-09-01 00:00:00|            390.74| 5417.043333333333|\n",
            "|          2|2023-10-01 00:00:00|            7999.4| 7762.833333333333|\n",
            "|          2|2023-11-01 00:00:00|            777.63|3055.9233333333327|\n",
            "+-----------+-------------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#27. Calculate rolling averages of sales amounts for each category over a 3-month window.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.functions import date_trunc\n",
        "\n",
        "# Create a window specification\n",
        "windowSpec = Window.partitionBy(\"category_id\").orderBy(\"month\").rowsBetween(-2, 0)\n",
        "\n",
        "# Calculate the rolling average\n",
        "rolling_avg = orders_with_categories.withColumn(\"month\", date_trunc(\"MM\", col(\"order_date\"))) \\\n",
        "    .groupBy(\"category_id\", \"month\") \\\n",
        "    .agg(_sum(\"total_amount\").alias(\"total_sales\")) \\\n",
        "    .withColumn(\"rolling_avg\", avg(\"total_sales\").over(windowSpec))\n",
        "\n",
        "rolling_avg.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9laF1H3Pqj_",
        "outputId": "c42013f4-0098-4793-bd70-6a4f126f0ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- category: string (nullable = true)\n",
            " |-- products: array (nullable = true)\n",
            " |    |-- element: map (containsNull = true)\n",
            " |    |    |-- key: string\n",
            " |    |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+-----------+------------------------------------------------------------------+\n",
            "|category   |products                                                          |\n",
            "+-----------+------------------------------------------------------------------+\n",
            "|Electronics|[{name -> Laptop, price -> 1200}, {name -> Keyboard, price -> 75}]|\n",
            "|Clothing   |[{name -> Shirt, price -> 50}, {name -> Jeans, price -> 100}]     |\n",
            "+-----------+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#28. Process nested JSON structures representing product hierarchies within categories.\n",
        "data = [{\"category\": \"Electronics\",\n",
        "         \"products\": [{\"name\": \"Laptop\", \"price\": 1200},\n",
        "                      {\"name\": \"Keyboard\", \"price\": 75}]},\n",
        "        {\"category\": \"Clothing\",\n",
        "         \"products\": [{\"name\": \"Shirt\", \"price\": 50},\n",
        "                      {\"name\": \"Jeans\", \"price\": 100}]}]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLOcX6doPwa3"
      },
      "outputs": [],
      "source": [
        "#29. Simulate streaming data ingestion for new orders and calculate real-time sales metrics.\n",
        "from pyspark.sql.functions import expr\n",
        "from time import sleep\n",
        "\n",
        "# Create a streaming DataFrame\n",
        "streaming_orders_df = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load() \\\n",
        "    .withColumn(\"customer_id\", expr(\"cast(value % 25 as int) + 1\")) \\\n",
        "    .withColumn(\"order_date\", expr(\"current_date()\")) \\\n",
        "    .withColumn(\"total_amount\", expr(\"abs(cast(value as int) % 5000) + 50\")) \\\n",
        "    .withColumn(\"status\", expr(\"case when value % 3 = 0 then 'completed' when value % 3 = 1 then 'pending' else 'cancelled' end\"))\n",
        "\n",
        "# Calculate real-time sales metrics\n",
        "real_time_sales = streaming_orders_df.groupBy(\"status\").agg(_sum(\"total_amount\").alias(\"total_sales\"))\n",
        "\n",
        "# Write the output to the console\n",
        "query = real_time_sales.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
        "\n",
        "# Keep the query running for a while\n",
        "try:\n",
        "    query.awaitTermination(10)  # Run for 10 seconds\n",
        "except KeyboardInterrupt:\n",
        "    query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0BqAvq3P2LB",
        "outputId": "46c84463-a1c8-4372-a7ce-d9786fe0ba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange hashpartitioning(order_date#224, country#214, 4), REPARTITION_BY_NUM, [plan_id=14223]\n",
            "   +- SortMergeJoin [customer_id#223L], [id#212L], Inner\n",
            "      :- Sort [customer_id#223L ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(customer_id#223L, 10), ENSURE_REQUIREMENTS, [plan_id=14218]\n",
            "      :     +- Project [id#222L, customer_id#223L, order_date#224, total_amount#225, status#226, dayofweek(order_date#224) AS dayofweek#2005, weekofyear(order_date#224) AS weekofyear#2012, month(order_date#224) AS month#2020, year(order_date#224) AS year#2029]\n",
            "      :        +- Filter isnotnull(customer_id#223L)\n",
            "      :           +- Scan ExistingRDD[id#222L,customer_id#223L,order_date#224,total_amount#225,status#226]\n",
            "      +- Sort [id#212L ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(id#212L, 10), ENSURE_REQUIREMENTS, [plan_id=14219]\n",
            "            +- Filter isnotnull(id#212L)\n",
            "               +- Scan ExistingRDD[id#212L,name#213,country#214,status#215,join_date#216]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#30. Optimize partition strategy for the orders table using order_date and country.\n",
        "orders_with_customers = orders_df.join(customers_df, orders_df.customer_id == customers_df.id)\n",
        "partitioned_orders = orders_with_customers.repartition(4, \"order_date\", \"country\")\n",
        "\n",
        "partitioned_orders.explain()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
